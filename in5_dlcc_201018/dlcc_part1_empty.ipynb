{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "```\n",
    "jupyter nbconvert dlcc_part1.ipynb --to slides --post serve --ServePostProcessor.port=8889 --SlidesExporter.reveal_scroll=True\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Learning Crash Course, part 1\n",
    "# Automatic Differentiation or Autograd\n",
    "\n",
    "| Pavel Nesterov  | Data Scientist  |\n",
    "|---|---|\n",
    "| <img src=\"img/ods.png\" />  | <img src=\"img/r2.png\" />  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Next workshops\n",
    "## Part 2: introduction to PyTorch\n",
    "- Vitalii Duk\n",
    "- 27 Oct\n",
    "\n",
    "## Part 3: real life deep learning\n",
    "- Evgenii Makarov\n",
    "- 10 Nov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Crash Course\n",
    "## Workshops != Lectures\n",
    "- we expect all of you to take part in the process\n",
    "  - answer questions\n",
    "  - solve problems\n",
    "  - write code\n",
    "  \n",
    "- `git clone `https://github.com/mephistopheies/dds\n",
    "  - folder `in5_dlcc_201018`\n",
    "- slack https://goo.gl/rbMgZV\n",
    "  - channel `dlcc18`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Content\n",
    "- ML reminder\n",
    "- Logistic Regression using SGD\n",
    "- **BREAK**\n",
    "- Autograd, single variable\n",
    "- Autograd, multivariable\n",
    "- Logistic Regression using Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ML reminder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/ds.png\" width=\"640\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/ds2.png\" width=\"640\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"./img/bengio.png\" width=\"640\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Machine Learning\n",
    "\n",
    "- **ML** is a subfield of **AI** that provides computers with the ability to learn without being explicitly programmed\n",
    "- _What does learning mean?_\n",
    "  - We say, that a program can learn from a data relative to some class of tasks $T$ and loss function $\\mathcal{L}$, if a quality of a solution increases (relatively to $\\mathcal{L}$) with increasing amount of data in a train set (maybe until some asymptote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Major ML tasks\n",
    "* *Supervised learning* - ML task of inferring a function $f: X \\rightarrow Y$ from labeled data; each sample is a pair of feature vector and some desired output value $D = \\left\\{ \\left( x_i, y_i \\right) \\right\\}_{i=1, \\ldots, n}$\n",
    "    * categorical - classification\n",
    "    * continuous - regression\n",
    "    * ordinal - ordinal regression, ranking\n",
    "        \n",
    "* *Unsupervised learning* - ML task of inferring a function to describe hidden structure from unlabeled data $D = \\left\\{ x_i \\right\\}_{i=1, \\ldots, n}$\n",
    "    * clustering - split data into several groups    \n",
    "    * dimensionality reduction - reduce number of features minimizing inforamation loss\n",
    "    * matrix complition - collaborative filtering, reccomender system\n",
    "    * semi-supervised learning - when you are given with small set of labeled data and large set of unlabeled\n",
    "\n",
    "* *Reinforcement learning* - ML task where agent learn optimal behaviour from his actions and response from environment\n",
    "    * differs from standard supervised learning in that correct input/output pairs are never presented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"./img/ml_tasks.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Minor ML tasks\n",
    "* *Transfer learning* - ML task that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem\n",
    "\n",
    "* *Active learning* - ML task, often called as optimal experimental design, where model can query environment for new labeled or unlabeled data, goal is to achieve good quality with minimum queries\n",
    "* *Online learning* - ML task, when there is a stream of data and domain can shift with time\n",
    "* *Meta-learning* or *learning-to-learn*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Predictive model\n",
    "\n",
    "*Predictive model* - is a parametric family of functions (hypothesis):\n",
    "\n",
    "$$\\large \\mathcal{H} = \\left\\{ h\\left(x, \\theta\\right) | \\theta \\in \\Theta \\right\\}$$\n",
    "\n",
    "* where\n",
    "    * $\\large h: X \\times \\Theta \\rightarrow Y$    \n",
    "    * $\\large \\Theta$ - is a set of parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Learning algorithm\n",
    "\n",
    "*Learning algorithm* - is a map from dataset to hypothesis set:\n",
    "\n",
    "$$\\large \\mathcal{M}: \\left(X \\times Y\\right)^n \\rightarrow \\mathcal{H}$$\n",
    "\n",
    "Ususally there are two steps in supervised learning tasks:\n",
    "1. Training step, when we train hypothesis: $\\large h = \\mathcal{M}\\left(D\\right)$\n",
    "* Testing step, when for given sample $\\large x$ we calculate output $\\large \\hat{y} = h\\left(x\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Empirical risk minimization\n",
    "*Empirical risk minimization* - is a principle in statistical learning theory for solving supervised learning tasks including regression and classification.\n",
    "\n",
    "Lets define real-valued loss function:\n",
    "$$\\large L: Y \\times Y \\rightarrow \\mathbb{R}$$\n",
    "which measures how different the prediction $\\large \\hat {y}$ of a hypothesis is from the true outcome $\\large y$.\n",
    "\n",
    "Then the risk associated with hypothesis $\\large h$ is then defined as the expectation of the loss function:\n",
    "$$\\large \\begin{array}{rcl}Q\\left(h\\right) &=& \\text{E}_{x, y \\sim P\\left(x, y\\right)}\\left[L\\left(h\\left(x\\right), y\\right)\\right] \\\\\n",
    "&=& \\int L\\left(h\\left(x\\right), y\\right) d P\\left(x, y\\right)\n",
    "\\end{array}$$\n",
    "\n",
    "Unfortunately $\\large P\\left(x, y\\right)$ is unknown to the learning algorithm. But we can compute an approximation, called empirical risk:\n",
    "\n",
    "$$\\large Q_{\\text{emp}}\\left(h\\right) = \\frac{1}{n} \\sum_{i=1}^n L\\left(h\\left(x_i\\right), y_i\\right)$$\n",
    "\n",
    "And principle says, that we should choose:\n",
    "$$\\large \\hat{h} = \\arg \\min_{h \\in \\mathcal{H}} Q_{\\text{emp}}\\left(h\\right)$$\n",
    "\n",
    "Common choices for loss-function:\n",
    "* classification: $\\large L\\left(\\hat{y}, y\\right) = \\text{I}\\left[\\hat{y} = y\\right]$\n",
    "  * cross-entropy as differentiable proxy: $\\large L\\left(\\hat{y}, y\\right) = -\\sum_{k=1}^K y \\cdot \\log \\hat{y}$\n",
    "* regression: $\\large L\\left(\\hat{y}, y\\right) = \\left(\\hat{y} - y\\right)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Differentiable model\n",
    "- $\\large h\\left(x, \\theta\\right)$ is a function of two arguments: features and parameters\n",
    "- henceforward we consider $\\large h\\left(x, \\theta\\right)$ to be differentiable\n",
    "  - _**what non-differentiable machine learning model do you know?**_\n",
    "  - _**what is the most popular algorithm for training/fitting differentiable models?**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gradient descent\n",
    "\n",
    "<img src=\"./img/gd.png\" width=\"640\"/>\n",
    "\n",
    "- For each sample $x$ repeat until convergence:\n",
    " - $\\large \\theta_{\\tau + 1} = \\theta_\\tau - \\eta \\dfrac{\\partial L}{\\partial \\theta}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gradient descent, local minima\n",
    "\n",
    "<img src=\"./img/gradient-local-minima.png\" width=\"640\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gradient descent, different modes\n",
    "\n",
    "<img src=\"./img/gd2.png\" width=\"640\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Logistic Regression\n",
    "\n",
    "## Predictive model\n",
    "- _**???**_\n",
    "\n",
    "## Loss function\n",
    "- _**???**_\n",
    "\n",
    "## Learning algorithm\n",
    "- _**???**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "ds = load_iris()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "for i in range(ds['data'].shape[1]):\n",
    "    ds['data'][:, i] = StandardScaler().fit_transform(ds['data'][:, i].reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "['setosa' 'versicolor' 'virginica']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2    50\n",
       "1    50\n",
       "0    50\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(ds.data.shape)\n",
    "print(ds.target_names)\n",
    "pd.Series(ds.target).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666 10.0\n",
      "0.6666666666666666 10.0\n",
      "0.6666666666666666 10.0\n",
      "0.6666666666666666 10.0\n",
      "0.6666666666666666 10.0\n",
      "0.6666666666666666 10.0\n",
      "0.6666666666666666 10.0\n",
      "0.6666666666666666 10.0\n",
      "0.6666666666666666 10.0\n",
      "0.6666666666666666 10.0\n",
      "0.6666666666666666 10.0\n",
      "0.6666666666666666 10.0\n",
      "0.6666666666666666 10.0\n",
      "0.6666666666666666 10.0\n",
      "0.6666666666666666 10.0\n",
      "0.6666666666666666 10.0\n",
      "0.6666666666666666 10.0\n",
      "0.6666666666666666 10.0\n",
      "0.6666666666666666 10.0\n",
      "0.6666666666666666 10.0\n",
      "0.6666666666666666 10.0\n",
      "0.6666666666666666 10.0\n",
      "0.6666666666666666 10.0\n",
      "0.6666666666666666 10.0\n",
      "0.6666666666666666 10.0\n",
      "0.6666666666666666 10.0\n",
      "0.6666666666666666 10.0\n",
      "0.6666666666666666 10.0\n",
      "0.6666666666666666 10.0\n",
      "0.6666666666666666 10.0\n",
      "0.6666666666666666 10.0\n",
      "0.6666666666666666 10.0\n",
      "0.6666666666666666 10.0\n",
      "0.6666666666666666 10.0\n",
      "0.6666666666666666 10.0\n",
      "0.6666666666666666 10.0\n",
      "0.6666666666666666 10.0\n",
      "0.6666666666666666 10.0\n",
      "0.6666666666666666 10.0\n",
      "0.6666666666666666 10.0\n",
      "0.6666666666666666 10.0\n",
      "0.6666666666666666 10.0\n",
      "0.6666666666666666 10.0\n",
      "0.6666666666666666 10.0\n",
      "0.6666666666666666 10.0\n",
      "0.6666666666666666 10.0\n",
      "0.6666666666666666 10.0\n",
      "0.6666666666666666 10.0\n",
      "0.6666666666666666 10.0\n",
      "0.6666666666666666 10.0\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "n_epochs = 50\n",
    "\n",
    "# parameters of the model\n",
    "# four for features and one is bias\n",
    "w = np.zeros(5)\n",
    "\n",
    "for _ in range(n_epochs):\n",
    "    # accuracy and loss of one epoch\n",
    "    acc_epoch = 0\n",
    "    loss_epoch = 0\n",
    "    for i in range(ds['data'].shape[0]):\n",
    "        x = ds['data'][i, :]\n",
    "        y = int(ds['target'][i] != 0)\n",
    "\n",
    "        # calculate argument of a sigmoid\n",
    "        z = 0 # <<<--- REPLACE IT\n",
    "        \n",
    "        # calculate value of the sigmoid\n",
    "        p = 0.5 # <<<--- REPLACE IT\n",
    "        \n",
    "        # make a decision\n",
    "        y_pred = int(p >= 0.5)\n",
    "        \n",
    "        # update accuracy\n",
    "        if y_pred == y:\n",
    "            acc_epoch += 1\n",
    "\n",
    "        # calculate and update loss value\n",
    "        loss_epoch += 10 # <<<--- REPLACE IT\n",
    "        \n",
    "        # update parameters of the model\n",
    "        w[0] -= 0 # <<<--- REPLACE IT\n",
    "        for i in range(x.shape[0]):\n",
    "            w[i + 1] -= 0 # <<<--- REPLACE IT\n",
    "        \n",
    "    # average accuracy and loss and print it\n",
    "    acc_epoch /= ds['data'].shape[0]\n",
    "    loss_epoch /= ds['data'].shape[0]\n",
    "    print(acc_epoch, loss_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"./img/cartoon-machine-learning-what-they-think.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"./img/ds_people.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Autograd, single variable\n",
    "\n",
    "- **Autograd** - provides classes and functions implementing automatic differentiation of arbitrary scalar valued functions. \n",
    "- We are going to implement our own autograd!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Computational graph\n",
    "- A computational graph is a directed acyclic graph where the nodes correspond to operations or variables.\n",
    "\n",
    "| Addition  | Affine transformation  |\n",
    "|---|---|\n",
    "| <img src=\"img/addition.png\" />  | <img src=\"img/affine_transformation.png\" />  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Quadratic function\n",
    "$$\\large f\\left(x\\right) = 4x^2 + 2x - 1$$\n",
    "\n",
    "$$\\large \\dfrac{\\partial f\\left(x\\right)}{\\partial x} = ???$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Quadratic function as Directed Acyclic Graph\n",
    "$$\\large f\\left(x\\right) = 4x^2 + 2x - 1$$\n",
    "\n",
    "<img src=\"img/qfg.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Chain rule\n",
    "\n",
    "$$\\Large\n",
    "\\begin{array}{rcl}\n",
    "\\left(f \\circ g\\right)' &=& \\left(f' \\circ g\\right)\\cdot g' \\\\\n",
    "\\dfrac{\\partial f\\left(g\\left(x\\right)\\right)}{\\partial x} &=& \\dfrac{\\partial f\\left(g\\left(x\\right)\\right)}{\\partial g\\left(x\\right)}\\cdot\\dfrac{\\partial g\\left(x\\right)}{\\partial x}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "<table width=\"100%\">\n",
    "<tr>\n",
    "<td>\n",
    "$$\\Large\n",
    "\\begin{array}{rcl}\n",
    "g_1\\left(x\\right) &=& 2x \\\\\n",
    "g_2\\left(x\\right) &=& x^2 \\\\\n",
    "g_3\\left(x\\right) &=& 4g_2 \\\\\n",
    "g_4\\left(x\\right) &=& g_3 + g_1 \\\\\n",
    "f\\left(x\\right) &=& g_4 - 1 \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "</td>\n",
    "<td>\n",
    "$$\\Large\n",
    "\\begin{array}{rcl}\n",
    "\\dfrac{\\partial g_1}{\\partial x} &=& ??? \\\\\n",
    "\\dfrac{\\partial g_2}{\\partial x} &=& ??? \\\\\n",
    "\\dfrac{\\partial g_3}{\\partial x} &=& ??? \\\\\n",
    "\\dfrac{\\partial g_4}{\\partial x} &=& ??? \\\\\n",
    "\\dfrac{\\partial f}{\\partial x} &=& ??? \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f = 9.00, d f = 1.00\n"
     ]
    }
   ],
   "source": [
    "class Variable:\n",
    "    \n",
    "    def __init__(self, value, derivative, name=None):\n",
    "        self.value = value\n",
    "        self.derivative = derivative\n",
    "        self.name = name\n",
    "\n",
    "    def __add__(self, other): \n",
    "        return Variable(\n",
    "            self.value + other.value,\n",
    "            self.derivative + other.derivative\n",
    "        )\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        # fill correct value and derivative\n",
    "        return Variable(\n",
    "            0, # <<<--- REPLACE IT\n",
    "            0  # <<<--- REPLACE  IT\n",
    "        )\n",
    "    \n",
    "    def __pow__(self, other):\n",
    "        # fill correct value and derivative\n",
    "        # assume that other is a number wrapped into variable\n",
    "        return Variable(\n",
    "            0, # <<<--- REPLACE IT\n",
    "            0  # <<<--- REPLACE IT\n",
    "        )\n",
    "\n",
    "    def __repr__(self):\n",
    "        name = 'f' if self.name is None else self.name\n",
    "        return '%s = %0.2f, d %s = %0.2f' % (name, self.value, name, self.derivative)\n",
    "    \n",
    "print(Variable(5, 1) + Variable(4, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "x = Variable(5, 1)\n",
    "v4 = Variable(4, 0)\n",
    "v2 = Variable(2, 0)\n",
    "vn1 = Variable(-1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f = 25.00, d f = 10.00"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x**v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f = 100.00, d f = 40.00"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v4*x**v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f = 10.00, d f = 2.00"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v2*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f = 109.00, d f = 42.00"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v4*x**v2 + v2*x + vn1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "x = Variable(5, 1)\n",
    "y = Variable(3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f = 15.00, d f = 8.00"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x*y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Autograd, multivariable\n",
    "\n",
    "## Quadratic function of two variables\n",
    "\n",
    "$$\\large f\\left(x, y\\right) = x^2 y + y^2$$\n",
    "\n",
    "$$\\large \\dfrac{\\partial}{\\partial x} f\\left(x, y\\right) = ???$$\n",
    "\n",
    "$$\\large \\dfrac{\\partial}{\\partial y} f\\left(x, y\\right) = ???$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table width=\"100%\">\n",
    "<tr>\n",
    "<td>\n",
    "<img src=\"img/qfg2.png\" />\n",
    "</td>\n",
    "<td>\n",
    "$$\\Large\n",
    "\\begin{array}{rcl}\n",
    "\\dfrac{\\partial g_1}{\\partial x} &=& ??? \\\\\n",
    "\\dfrac{\\partial g_2}{\\partial x} &=& ??? \\\\\n",
    "\\dfrac{\\partial g_3}{\\partial x} &=& ??? \\\\\n",
    "\\dfrac{\\partial f}{\\partial x} &=& ??? \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "</td>\n",
    "<td>\n",
    "$$\\Large\n",
    "\\begin{array}{rcl}\n",
    "\\dfrac{\\partial g_1}{\\partial y} &=& ??? \\\\\n",
    "\\dfrac{\\partial g_2}{\\partial y} &=& ??? \\\\\n",
    "\\dfrac{\\partial g_3}{\\partial y} &=& ??? \\\\\n",
    "\\dfrac{\\partial f}{\\partial y} &=& ??? \\\\\n",
    "\\end{array}\n",
    "$$    \n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable(\n",
      "  (x + y),\n",
      "  8.00,\n",
      "  d[(x + y)]/d[x]=1.00; d[(x + y)]/d[y]=1.00\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class Variable:\n",
    "    \n",
    "    \n",
    "    def __init__(self, name, value, derivative=None, is_number=False):\n",
    "        self.name = name\n",
    "        self.value = value\n",
    "        if derivative is None:\n",
    "            self.derivative = defaultdict(float) # returns zero for any absent key\n",
    "            if not is_number:\n",
    "                self.derivative[self.name] = 1\n",
    "        else:\n",
    "            self.derivative = derivative   \n",
    "        self.is_number = is_number\n",
    "\n",
    "        \n",
    "    def __add__(self, other):\n",
    "        derivative = defaultdict(float)\n",
    "        leafs = list(set(self.derivative.keys()).union(other.derivative.keys()))\n",
    "        for v in leafs:\n",
    "            derivative[v] = 1*self.derivative[v] + 1*other.derivative[v]\n",
    "        return Variable(\n",
    "            '(%s + %s)' % (self.name, other.name),\n",
    "            self.value + other.value,\n",
    "            derivative,\n",
    "            is_number=self.is_number and other.is_number\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def __mul__(self, other):\n",
    "        derivative = defaultdict(float)\n",
    "        leafs = list(set(self.derivative.keys()).union(other.derivative.keys()))\n",
    "        for v in leafs:\n",
    "            # write correct code\n",
    "            derivative[v] = 0 # <<<--- REPLACE  IT\n",
    "        return Variable(\n",
    "            '(%s * %s)' % (self.name, other.name),\n",
    "            self.value * other.value,\n",
    "            derivative,\n",
    "            is_number=self.is_number and other.is_number\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def __pow__(self, other):\n",
    "        if not other.is_number:\n",
    "            raise NotImplementedError()\n",
    "        derivative = defaultdict(float)\n",
    "        for k, v in self.derivative.items():\n",
    "            # write correct code\n",
    "            derivative[k] = 0 # <<<--- REPLACE  IT\n",
    "        return Variable(\n",
    "            '(%s^(%s))' % (self.name, other.name),\n",
    "            self.value**other.value,\n",
    "            derivative,\n",
    "            is_number=self.is_number\n",
    "        )\n",
    "    \n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'Variable(\\n  %s,\\n  %0.2f,\\n  %s\\n)' % (\n",
    "            self.name, \n",
    "            self.value,\n",
    "            '' if self.derivative is None else '; '.join(\n",
    "                [('d[%s]/d[%s]=%0.2f' % (self.name, k, v)) \n",
    "                 for (k, v) in self.derivative.items()]\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    \n",
    "print(Variable('x', 5) + Variable('y', 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "var_x = Variable('x', 5)\n",
    "var_y = Variable('y', 3)\n",
    "var_2 = Variable('2', 2, is_number=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable(\n",
       "  (x^(2)),\n",
       "  25.00,\n",
       "  d[(x^(2))]/d[x]=10.00\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_x**var_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable(\n",
       "  (y^(2)),\n",
       "  9.00,\n",
       "  d[(y^(2))]/d[y]=6.00\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_y**var_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable(\n",
       "  ((x^(2)) * y),\n",
       "  75.00,\n",
       "  d[((x^(2)) * y)]/d[x]=30.00; d[((x^(2)) * y)]/d[y]=25.00\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(var_x**var_2)*var_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable(\n",
       "  (((x^(2)) * y) + (y^(2))),\n",
       "  84.00,\n",
       "  d[(((x^(2)) * y) + (y^(2)))]/d[x]=30.00; d[(((x^(2)) * y) + (y^(2)))]/d[y]=31.00\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(var_x**var_2)*var_y + var_y**var_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"./img/gradmem.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Logistic Regression using Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "['setosa' 'versicolor' 'virginica']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2    50\n",
       "1    50\n",
       "0    50\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(ds.data.shape)\n",
    "print(ds.target_names)\n",
    "pd.Series(ds.target).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/lr_cg.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Existing functions\n",
    "- addition `__add__`\n",
    "- multiplication `__mul__`\n",
    "- power (of real number) `__pow__`\n",
    "\n",
    "## To do list\n",
    "- negation `__neg__`\n",
    "- subtraction `__sub__`\n",
    "- logarithm `log`\n",
    "- exponent `exp`\n",
    "- devision `__truediv__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable(\n",
       "  ((w0 * x0) + (w1 * x1)),\n",
       "  2.30,\n",
       "  d[((w0 * x0) + (w1 * x1))]/d[w0]=1.00; d[((w0 * x0) + (w1 * x1))]/d[w1]=5.00\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_w0 = Variable('w0', 0.3)\n",
    "var_w1 = Variable('w1', 0.4)\n",
    "\n",
    "var_x0 = Variable('x0', 1, is_number=True)\n",
    "var_x1 = Variable('x1', 5, is_number=True)\n",
    "\n",
    "var_z = var_w0*var_x0 + var_w1*var_x1\n",
    "\n",
    "var_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable(\n",
       "  (1 * ((1 + exp(-((w0 * x0) + (w1 * x1))))^(-1))),\n",
       "  0.91,\n",
       "  d[(1 * ((1 + exp(-((w0 * x0) + (w1 * x1))))^(-1)))]/d[w0]=0.08; d[(1 * ((1 + exp(-((w0 * x0) + (w1 * x1))))^(-1)))]/d[w1]=0.41\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_1 = Variable('1', 1, is_number=True)\n",
    "\n",
    "var_p = var_1 / (var_1 + (-var_z).exp())\n",
    "\n",
    "var_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable(\n",
       "  ((-1 * log((1 * ((1 + exp(-((w0 * x0) + (w1 * x1))))^(-1))))) + -((1 + -1) * log((1 + -(1 * ((1 + exp(-((w0 * x0) + (w1 * x1))))^(-1))))))),\n",
       "  0.10,\n",
       "  d[((-1 * log((1 * ((1 + exp(-((w0 * x0) + (w1 * x1))))^(-1))))) + -((1 + -1) * log((1 + -(1 * ((1 + exp(-((w0 * x0) + (w1 * x1))))^(-1)))))))]/d[w0]=-0.09; d[((-1 * log((1 * ((1 + exp(-((w0 * x0) + (w1 * x1))))^(-1))))) + -((1 + -1) * log((1 + -(1 * ((1 + exp(-((w0 * x0) + (w1 * x1))))^(-1)))))))]/d[w1]=-0.46\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_y = Variable('1', 1, is_number=True)\n",
    "\n",
    "var_loss = -var_y*var_p.log() - (var_1 - var_y)*(var_1 - var_p).log()\n",
    "\n",
    "var_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.09112296101485613, -0.45561480507428076)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_loss.derivative['w0'], var_loss.derivative['w1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Let's implement LogReg training loop now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96 0.6528111380885625\n",
      "0.9933333333333333 0.581164305957222\n",
      "0.9933333333333333 0.5226005581235463\n",
      "0.9933333333333333 0.4743056455314175\n",
      "0.9933333333333333 0.4340636663748048\n",
      "1.0 0.40016604202782347\n",
      "1.0 0.37130600766796784\n",
      "1.0 0.346484225642095\n",
      "1.0 0.3249328596030666\n",
      "1.0 0.3060575285873605\n",
      "1.0 0.28939400027234946\n",
      "1.0 0.2745762057522463\n",
      "1.0 0.2613126615741007\n",
      "1.0 0.24936903917553097\n",
      "1.0 0.2385552053295628\n",
      "1.0 0.2287155160287722\n",
      "1.0 0.21972148729741592\n",
      "1.0 0.21146621312568256\n",
      "1.0 0.20386007695509364\n",
      "1.0 0.1968274284964393\n",
      "1.0 0.19030398687057964\n",
      "1.0 0.18423479477171176\n",
      "1.0 0.1785725940934609\n",
      "1.0 0.173276526509504\n",
      "1.0 0.16831108655172247\n",
      "1.0 0.16364527236072607\n",
      "1.0 0.15925189230711104\n",
      "1.0 0.15510699537414815\n",
      "1.0 0.15118940045972906\n",
      "1.0 0.1474803052440368\n",
      "1.0 0.1439629594443854\n",
      "1.0 0.14062239047638905\n",
      "1.0 0.13744517200623366\n",
      "1.0 0.13441922779221577\n",
      "1.0 0.13153366470784816\n",
      "1.0 0.12877863001253645\n",
      "1.0 0.12614518886318687\n",
      "1.0 0.12362521879684547\n",
      "1.0 0.12121131850294167\n",
      "1.0 0.11889672867612236\n",
      "1.0 0.1166752631218156\n",
      "1.0 0.1145412485956187\n",
      "1.0 0.11248947210919319\n",
      "1.0 0.11051513464112879\n",
      "1.0 0.10861381036024279\n",
      "1.0 0.10678141060816855\n",
      "1.0 0.10501415200348552\n",
      "1.0 0.10330852812554762\n",
      "1.0 0.1016612843161585\n",
      "1.0 0.10006939520420091\n"
     ]
    }
   ],
   "source": [
    "var_w0 = Variable('w0', 0)\n",
    "var_w1 = Variable('w1', 0)\n",
    "var_w2 = Variable('w2', 0)\n",
    "var_w3 = Variable('w3', 0)\n",
    "var_w4 = Variable('w4', 0)\n",
    "\n",
    "learning_rate = 0.001\n",
    "n_epochs = 50\n",
    "var_1 = Variable('1', 1, is_number=True)\n",
    "\n",
    "for _ in range(n_epochs):\n",
    "    acc_epoch = 0\n",
    "    loss_epoch = 0\n",
    "    for i in range(ds['data'].shape[0]):\n",
    "        x = ds['data'][i, :]\n",
    "        y = int(ds['target'][i] != 0)\n",
    "\n",
    "        var_y = Variable(str(y), y, is_number=True)\n",
    "\n",
    "        var_z = \\\n",
    "            var_w0 + \\\n",
    "            var_w1*Variable('x1', x[0], is_number=True) + \\\n",
    "            var_w2*Variable('x2', x[1], is_number=True) + \\\n",
    "            var_w3*Variable('x3', x[2], is_number=True) + \\\n",
    "            var_w4*Variable('x4', x[3], is_number=True)\n",
    "\n",
    "        var_p = var_1/(var_1 + (-var_z).exp())\n",
    "            \n",
    "        y_pred = int(var_p.value >= 0.5)\n",
    "        if y_pred == y:\n",
    "            acc_epoch += 1\n",
    "\n",
    "        if y == 0:\n",
    "            var_loss = -(var_1 - var_p).log()\n",
    "        else:\n",
    "            var_loss = -var_p.log()\n",
    "        loss_epoch += var_loss.value                \n",
    "\n",
    "        var_w0.value -= learning_rate*var_loss.derivative[var_w0.name]\n",
    "        var_w1.value -= learning_rate*var_loss.derivative[var_w1.name]\n",
    "        var_w2.value -= learning_rate*var_loss.derivative[var_w2.name]\n",
    "        var_w3.value -= learning_rate*var_loss.derivative[var_w3.name]\n",
    "        var_w4.value -= learning_rate*var_loss.derivative[var_w4.name]\n",
    "\n",
    "    acc_epoch /= ds['data'].shape[0]\n",
    "    loss_epoch /= ds['data'].shape[0]\n",
    "    print(acc_epoch, loss_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"./img/future-continued.jpg\" />"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
